{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Term Weighting | tf-idf"
      ],
      "metadata": {
        "id": "_A1KQoUdARy1"
      },
      "id": "_A1KQoUdARy1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46939cc1-75e3-4b05-ac7e-2fb54efaf05d",
      "metadata": {
        "id": "46939cc1-75e3-4b05-ac7e-2fb54efaf05d",
        "outputId": "e23f79d4-a87a-4284-8bdb-464d4c0b4e63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -umpy (c:\\users\\gitsa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -umpy (c:\\users\\gitsa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -umpy (c:\\users\\gitsa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -umpy (c:\\users\\gitsa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -umpy (c:\\users\\gitsa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -umpy (c:\\users\\gitsa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -umpy (c:\\users\\gitsa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -umpy (c:\\users\\gitsa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
            "WARNING: You are using pip version 21.2.4; however, version 22.0.4 is available.\n",
            "You should consider upgrading via the 'c:\\users\\gitsa\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Using cached nltk-3.7-py3-none-any.whl (1.5 MB)\n",
            "Requirement already satisfied: click in c:\\users\\gitsa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (7.1.2)\n",
            "Collecting regex>=2021.8.3\n",
            "  Using cached regex-2022.3.15-cp39-cp39-win_amd64.whl (274 kB)\n",
            "Requirement already satisfied: joblib in c:\\users\\gitsa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (1.0.1)\n",
            "Requirement already satisfied: tqdm in c:\\users\\gitsa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (4.61.1)\n",
            "Installing collected packages: regex, nltk\n",
            "Successfully installed nltk-3.7 regex-2022.3.15\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b2ad709-fc3e-4fee-ba9e-254e5baf9321",
      "metadata": {
        "id": "2b2ad709-fc3e-4fee-ba9e-254e5baf9321"
      },
      "source": [
        "**Loading Modules**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd5a1423-2370-4bc9-9c18-739b12b289d9",
      "metadata": {
        "id": "fd5a1423-2370-4bc9-9c18-739b12b289d9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import json\n",
        "import string\n",
        "import re\n",
        "import math\n",
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ea2e27e-85fc-4b7d-aab8-e9b8581626c3",
      "metadata": {
        "id": "1ea2e27e-85fc-4b7d-aab8-e9b8581626c3",
        "outputId": "4b1e3cea-ac3f-40e9-b30f-87e7e8106863"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\gitsa\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\gitsa\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7298fb30-cf3a-4f0a-a132-82d7f7e8b6d9",
      "metadata": {
        "id": "7298fb30-cf3a-4f0a-a132-82d7f7e8b6d9"
      },
      "source": [
        "**Preprocessing Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37b3d6b0-2a6e-4e90-baa7-f8244118ae90",
      "metadata": {
        "id": "37b3d6b0-2a6e-4e90-baa7-f8244118ae90"
      },
      "outputs": [],
      "source": [
        "def preprocess(input):\n",
        "    input=input.replace('\\a',' ')\n",
        "    input=input.replace('\\b',' ')\n",
        "    input=input.replace('\\f',' ')\n",
        "    input=input.replace('\\n',' ')    \n",
        "    input=input.replace('\\r',' ')\n",
        "    input=input.replace('\\t',' ')\n",
        "    input=input.replace('\\v',' ')\n",
        "    # convert to lower case\n",
        "    output = input.lower()\n",
        "    # remove punctuations\n",
        "    punctuations=string.punctuation.replace(\"'\",'')\n",
        "    output = \"\".join([char if char not in punctuations else ' ' for char in output])\n",
        "    output = output.replace(\"'\",'')\n",
        "    # tokenize\n",
        "    output = nltk.word_tokenize(output)\n",
        "    # removing words with special characters\n",
        "    output = [word for word in output if re.sub(r'[^\\x20-\\x7e]','',word) == word]\n",
        "    # remove stopwords and numeric tokens\n",
        "    output = [word.strip() for word in output if word not in nltk.corpus.stopwords.words('english') and not word.isnumeric()]\n",
        "    # stemming\n",
        "    # output = [PorterStemmer().stem(word) for word in output]\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c0e2fd9-e71b-48e9-aaf4-b22e5a9422a9",
      "metadata": {
        "id": "8c0e2fd9-e71b-48e9-aaf4-b22e5a9422a9"
      },
      "outputs": [],
      "source": [
        "# preprocessing\n",
        "for doc in raw_data:\n",
        "    raw_data[doc] = preprocess(raw_data[doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8dc834e-4c65-4d50-b426-fcbdd3021d71",
      "metadata": {
        "id": "c8dc834e-4c65-4d50-b426-fcbdd3021d71"
      },
      "source": [
        "**Loading already preprocessed file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23dd948c-473c-4cb4-af09-20797aeb36a2",
      "metadata": {
        "id": "23dd948c-473c-4cb4-af09-20797aeb36a2"
      },
      "outputs": [],
      "source": [
        "raw_data = json.load(open(\"PycharmProjects/class12/preprocessed.json\", \"r\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "862aad3c-3e6c-4d0c-8dda-3e29b9c6a21c",
      "metadata": {
        "id": "862aad3c-3e6c-4d0c-8dda-3e29b9c6a21c"
      },
      "source": [
        "**doc to doc-id mapping**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca5b6527-6a3b-44dd-bde1-69ca3c335fa9",
      "metadata": {
        "id": "ca5b6527-6a3b-44dd-bde1-69ca3c335fa9"
      },
      "outputs": [],
      "source": [
        "def map_docs(raw_data):\n",
        "    doc_ids = {}\n",
        "    id = 1\n",
        "    for doc in raw_data:\n",
        "        doc_ids[doc] = id\n",
        "        id += 1\n",
        "    return doc_ids\n",
        "\n",
        "doc_ids = map_docs(raw_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cd57fad-ea69-41de-b745-9e5136ba1230",
      "metadata": {
        "id": "1cd57fad-ea69-41de-b745-9e5136ba1230"
      },
      "source": [
        "**Creating unigram inverted index**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23747670-9fae-4a13-9443-d46b431629fd",
      "metadata": {
        "id": "23747670-9fae-4a13-9443-d46b431629fd"
      },
      "outputs": [],
      "source": [
        "def create_index(doc_ids):\n",
        "    index = {}\n",
        "    for doc in doc_ids:\n",
        "        for token in raw_data[doc]:\n",
        "            # if token exists in index, add doc id\n",
        "            if token in index.keys():\n",
        "                index[token][1].add(doc_ids[doc])\n",
        "                index[token][0] = len(index[token][1])\n",
        "            # if token does not exist in index, add to index\n",
        "            else:\n",
        "                index[token] = [1, {doc_ids[doc]}]\n",
        "    return index\n",
        "\n",
        "index = create_index(doc_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f9be36e-ffca-4a38-9c6f-8feadf90056b",
      "metadata": {
        "id": "9f9be36e-ffca-4a38-9c6f-8feadf90056b"
      },
      "source": [
        "**Finding idf values of words**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbae8d31-855b-444c-9327-c9bca850f0d1",
      "metadata": {
        "id": "cbae8d31-855b-444c-9327-c9bca850f0d1"
      },
      "outputs": [],
      "source": [
        "idfValue={}\n",
        "totalLen=len(doc_ids)\n",
        "for word in index:\n",
        "    idfValue[word] = math.log2(totalLen/(index[word][0]+1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f1d23b0-8e4b-4a1a-99da-7a12a74cdbc4",
      "metadata": {
        "id": "8f1d23b0-8e4b-4a1a-99da-7a12a74cdbc4"
      },
      "source": [
        "Storing terms of a doc in dictionary manner for easier access, storing doc length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f174fc78-b07b-401c-867f-0287d5962813",
      "metadata": {
        "id": "f174fc78-b07b-401c-867f-0287d5962813"
      },
      "outputs": [],
      "source": [
        "doc_terms={}\n",
        "doc_len={}\n",
        "ctr=0\n",
        "for doc in raw_data:\n",
        "    ctr+=1\n",
        "    terms=raw_data[doc]\n",
        "    setterms=set(terms)\n",
        "    temp_dict={}\n",
        "    for word in setterms:\n",
        "        temp_dict[word]=terms.count(word)\n",
        "    # print(ctr,len(raw_data[doc]))\n",
        "    doc_terms[doc_ids[doc]]=temp_dict\n",
        "    doc_len[doc_ids[doc]]=len(terms)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3aebe3e7-340c-449c-90e9-9a9273d1989d",
      "metadata": {
        "id": "3aebe3e7-340c-449c-90e9-9a9273d1989d"
      },
      "source": [
        "Storing max frequency in a doc for double normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cadec05-87b3-4f2d-96c9-71329dca6850",
      "metadata": {
        "id": "4cadec05-87b3-4f2d-96c9-71329dca6850"
      },
      "outputs": [],
      "source": [
        "doc_max={}\n",
        "for doc in doc_terms:\n",
        "    maxa=0\n",
        "    for word in doc_terms[doc]:\n",
        "        maxa=max(maxa,doc_terms[doc][word])\n",
        "    doc_max[doc]=maxa"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56d8d3ad-a195-485a-bd26-da2c45a1770b",
      "metadata": {
        "id": "56d8d3ad-a195-485a-bd26-da2c45a1770b"
      },
      "source": [
        "Finding tf-idf values by iterating over every word for all docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "740ef4ec-fcd4-4ec9-a15b-10627c53df14",
      "metadata": {
        "id": "740ef4ec-fcd4-4ec9-a15b-10627c53df14"
      },
      "outputs": [],
      "source": [
        "def tfvalue(docs, idf, doc_len,doc_max):\n",
        "    bin = {}\n",
        "    for doc in docs:\n",
        "        bin[doc] = {}\n",
        "    ctr = 0\n",
        "    for word in idf:\n",
        "        idfval = idf[word]\n",
        "        for doc in docs:\n",
        "            if word in docs[doc].keys():\n",
        "                val = docs[doc][word]\n",
        "                bin[doc][word] = [idfval, val * idfval, val * idfval / doc_len[doc], math.log2(1 + val) * idfval,\n",
        "                                  idfval*(float((0.5+0.5*(val)/doc_max[doc])))]\n",
        "            else:\n",
        "                bin[doc][word] = [0, 0, 0, 0, float(0.5*idfval)]\n",
        "        ctr += 1\n",
        "        if ctr % 1000 == 0:\n",
        "            print(ctr)\n",
        "    return bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "692226f9-f289-43eb-a2f2-9a0977159d26",
      "metadata": {
        "tags": [],
        "id": "692226f9-f289-43eb-a2f2-9a0977159d26",
        "outputId": "ddb857c4-4432-400c-c77d-1e7a313c73d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "30000\n",
            "31000\n",
            "32000\n",
            "33000\n",
            "34000\n",
            "35000\n",
            "36000\n",
            "37000\n",
            "38000\n",
            "39000\n",
            "40000\n",
            "41000\n",
            "42000\n",
            "43000\n",
            "44000\n",
            "45000\n",
            "46000\n",
            "47000\n",
            "48000\n",
            "49000\n",
            "50000\n",
            "51000\n",
            "52000\n",
            "53000\n",
            "54000\n",
            "55000\n",
            "56000\n",
            "57000\n",
            "58000\n",
            "59000\n",
            "60000\n",
            "61000\n",
            "62000\n",
            "63000\n",
            "64000\n",
            "65000\n",
            "66000\n",
            "67000\n"
          ]
        }
      ],
      "source": [
        "tfidf =tfvalue(doc_terms,idfValue,doc_len,doc_max)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7abfabf-e245-4d06-bffa-c2b067a20f89",
      "metadata": {
        "tags": [],
        "id": "a7abfabf-e245-4d06-bffa-c2b067a20f89"
      },
      "source": [
        "Functions to take input a query, preprocess it, and find its tf-idf value and rank documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6078965-d7d3-43fd-9ff0-421cae709a52",
      "metadata": {
        "id": "c6078965-d7d3-43fd-9ff0-421cae709a52"
      },
      "outputs": [],
      "source": [
        "def scoring(query,doc,tfidf,j):\n",
        "    val=0\n",
        "    for word in query:\n",
        "        val+=tfidf[doc][word][j]\n",
        "    return val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ab49c6f-792a-463d-9c6a-ef90b6ee5162",
      "metadata": {
        "id": "4ab49c6f-792a-463d-9c6a-ef90b6ee5162"
      },
      "outputs": [],
      "source": [
        "def process(input,j):\n",
        "    global tfidf,doc_ids\n",
        "    terms = preprocess(input)\n",
        "    query = set(terms)\n",
        "\n",
        "    best_docs = []\n",
        "    for doc in doc_ids:\n",
        "        coeff = scoring(query, doc_ids[doc],tfidf,j)\n",
        "        best_docs.append([coeff, doc])\n",
        "    # finding 5 best documents\n",
        "    best_docs.sort(reverse = True)\n",
        "    return best_docs[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ee4ab66-6be5-44a9-bd6c-5fe41bc3925b",
      "metadata": {
        "id": "8ee4ab66-6be5-44a9-bd6c-5fe41bc3925b"
      },
      "outputs": [],
      "source": [
        "\n",
        "format={0:'Binary Scheme',1:'Raw count Scheme',2:'Term frequency Scheme',3:'Log normalization Scheme',4:'Double normalization Scheme'}\n",
        "# input and output\n",
        "def run():\n",
        "    n=int(input('Enter number of queries:'))\n",
        "    for i in range(n):\n",
        "        query = input(\"Input query: \")\n",
        "        for j in range(5):\n",
        "            result = process(query,j)\n",
        "            print('\\nFor',format[j])\n",
        "            print(result)\n",
        "            print('Top 5 documents:')\n",
        "            for r in result:\n",
        "                print(r[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e14fd6e3-8cc6-4fb7-9a9b-b308e34ca09d",
      "metadata": {
        "id": "e14fd6e3-8cc6-4fb7-9a9b-b308e34ca09d",
        "outputId": "95492afd-7015-40f9-a0d4-dba748973beb"
      },
      "outputs": [
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            "Enter number of queries: 1\n",
            "Input query:  first aid the\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "For Binary Scheme\n",
            "[[4.791458074186427, 'woodbugs.txt'], [4.791458074186427, 'wagit.txt'], [4.791458074186427, 'urban.txt'], [4.791458074186427, 'turbo.hum'], [4.791458074186427, 'tfepisod.hum']]\n",
            "Top 5 documents:\n",
            "woodbugs.txt\n",
            "wagit.txt\n",
            "urban.txt\n",
            "turbo.hum\n",
            "tfepisod.hum\n",
            "\n",
            "For Raw count Scheme\n",
            "[[74.58699718057298, 'mlverb.hum'], [48.12993557780939, 'practica.txt'], [40.37164436755816, 'hackingcracking.txt'], [39.68922762737971, 'candy.txt'], [34.89776955319328, 'humor9.txt']]\n",
            "Top 5 documents:\n",
            "mlverb.hum\n",
            "practica.txt\n",
            "hackingcracking.txt\n",
            "candy.txt\n",
            "humor9.txt\n",
            "\n",
            "For Term frequency Scheme\n",
            "[[0.05578241921279136, 'hum2'], [0.04395833095583879, '1st_aid.txt'], [0.0329751494841258, 'whoon1st.hum'], [0.0329751494841258, 'abbott.txt'], [0.03205905458781499, 'labels.txt']]\n",
            "Top 5 documents:\n",
            "hum2\n",
            "1st_aid.txt\n",
            "whoon1st.hum\n",
            "abbott.txt\n",
            "labels.txt\n",
            "\n",
            "For Log normalization Scheme\n",
            "[[13.350112533012803, 'mlverb.hum'], [12.919408028890427, 'insult.lst'], [12.646052707706064, 'practica.txt'], [10.864887361886442, 'candy.txt'], [10.831722102821628, 'insults1.txt']]\n",
            "Top 5 documents:\n",
            "mlverb.hum\n",
            "insult.lst\n",
            "practica.txt\n",
            "candy.txt\n",
            "insults1.txt\n",
            "\n",
            "For Double normalization Scheme\n",
            "[[4.791458074186427, 'woodbugs.txt'], [4.791458074186427, 'wagit.txt'], [4.791458074186427, 'urban.txt'], [4.791458074186427, 'turbo.hum'], [4.791458074186427, 'tfepisod.hum']]\n",
            "Top 5 documents:\n",
            "woodbugs.txt\n",
            "wagit.txt\n",
            "urban.txt\n",
            "turbo.hum\n",
            "tfepisod.hum\n"
          ]
        }
      ],
      "source": [
        "run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03e93790-c915-4fea-8caa-49dcca08d23f",
      "metadata": {
        "id": "03e93790-c915-4fea-8caa-49dcca08d23f"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "name": "TermWeighting.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}