{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes"
      ],
      "metadata": {
        "id": "f3kE5q5oCVM5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdWHo_V5jgHw"
      },
      "source": [
        "**Loading Modules**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "766mshFGuV5j"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import json\n",
        "import string\n",
        "import re\n",
        "from nltk.stem import PorterStemmer\n",
        "import os\n",
        "import spacy\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRoRV9-XRejC"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1m_iPH8uxNZ",
        "outputId": "597a906c-2f4b-4cee-be64-bded5497aad8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUAGcY9OuzHA",
        "outputId": "0cb94424-2b80-4d7f-e769-a2152cc77cf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VRu6uRCjkbw"
      },
      "source": [
        "**Preprocessing function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOzrxaWLu51b"
      },
      "outputs": [],
      "source": [
        "def preprocess(input):\n",
        "    input=input.replace('\\a',' ')\n",
        "    input=input.replace('\\b',' ')\n",
        "    input=input.replace('\\f',' ')\n",
        "    input=input.replace('\\n',' ')    \n",
        "    input=input.replace('\\r',' ')\n",
        "    input=input.replace('\\t',' ')\n",
        "    input=input.replace('\\v',' ')\n",
        "    # removing special characters\n",
        "    # output = re.sub(r'[^\\x20-\\x7e]','',input)\n",
        "    # print(output)\n",
        "    # convert to lower case\n",
        "    output = input.lower()\n",
        "    # remove punctuations\n",
        "    punctuations=string.punctuation.replace(\"'\",'')\n",
        "    output = \"\".join([char if char not in punctuations else ' ' for char in output])\n",
        "    output = output.replace(\"'\",'')\n",
        "    # output = \"\".join([char for char in output if char not in \"'\"])\n",
        "    # output = \" \".join([char for char in output if char not in string.punctuation.replace(\"'\", \"\")])\n",
        "    # tokenize  \n",
        "    output = nltk.WhitespaceTokenizer().tokenize(output)\n",
        "    # removing words with special characters\n",
        "    output = [word for word in output if re.sub(r'[^\\x20-\\x7e]','',word) == word]\n",
        "    # remove stopwords and numeric tokens and doing lemmitization\n",
        "    proc= spacy.tokens.Doc(nlp.vocab,words=output)\n",
        "    output = [word.lemma_ for word in proc if word.lemma_ not in nltk.corpus.stopwords.words('english') and not word.lemma_.isnumeric()]\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eW_znjt-jsdG"
      },
      "source": [
        "**Loading Data from different Classes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSggZpBIu6ZD",
        "outputId": "910f0178-2921-4606-f49c-eaf894481d70"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['sci.space',\n",
              " 'talk.politics.misc',\n",
              " 'comp.graphics',\n",
              " 'rec.sport.hockey',\n",
              " 'sci.med']"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(os.listdir('/content/drive/MyDrive/IR_Assignments/20_newsgroups/'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMIXcVmTxhic"
      },
      "outputs": [],
      "source": [
        "graphics=os.listdir('/content/drive/MyDrive/IR_Assignments/20_newsgroups/comp.graphics')\n",
        "sports=os.listdir('/content/drive/MyDrive/IR_Assignments/20_newsgroups/rec.sport.hockey')\n",
        "med=os.listdir('/content/drive/MyDrive/IR_Assignments/20_newsgroups/sci.med')\n",
        "space=os.listdir('/content/drive/MyDrive/IR_Assignments/20_newsgroups/sci.space')\n",
        "politics=os.listdir('/content/drive/MyDrive/IR_Assignments/20_newsgroups/talk.politics.misc')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mWKJGcavU74"
      },
      "outputs": [],
      "source": [
        "graphicsData={}\n",
        "sportsData={}\n",
        "medData={}\n",
        "spaceData={}\n",
        "politicsData={}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xci7hcOKyTBW",
        "outputId": "f829e129-8a1d-4c3d-a0b8-07e73ecc6785"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n"
          ]
        }
      ],
      "source": [
        "ctr=0\n",
        "for file in graphics:\n",
        "  ctr+=1\n",
        "  with open('/content/drive/MyDrive/IR_Assignments/20_newsgroups/comp.graphics/'+file) as doc:\n",
        "    try:\n",
        "      graphicsData[file]=doc.read()\n",
        "\n",
        "    except:\n",
        "      with open('/content/drive/MyDrive/IR_Assignments/20_newsgroups/comp.graphics/'+str(file),encoding='ISO-8859-1') as doctry:\n",
        "        graphicsData[file]=doctry.read()\n",
        "  if ctr%100==0:\n",
        "    print(ctr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14aj8493y2NE",
        "outputId": "419f7d84-448f-4ac0-d723-5ef686c882b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n"
          ]
        }
      ],
      "source": [
        "ctr=0\n",
        "for file in space:\n",
        "  ctr+=1\n",
        "  with open('/content/drive/MyDrive/IR_Assignments/20_newsgroups/sci.space/'+file) as doc:\n",
        "    try:\n",
        "      spaceData[file]=doc.read()\n",
        "\n",
        "    except:\n",
        "      with open('/content/drive/MyDrive/IR_Assignments/20_newsgroups/sci.space/'+str(file),encoding='ISO-8859-1') as doctry:\n",
        "        spaceData[file]=doctry.read()\n",
        "  if ctr%100==0:\n",
        "    print(ctr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbC6VhLhGUYh",
        "outputId": "7024058e-71db-4b8f-b5e5-714b548a584a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n"
          ]
        }
      ],
      "source": [
        "ctr=0\n",
        "for file in politics:\n",
        "  ctr+=1\n",
        "  with open('/content/drive/MyDrive/IR_Assignments/20_newsgroups/talk.politics.misc/'+file) as doc:\n",
        "    try:\n",
        "      politicsData[file]=doc.read()\n",
        "\n",
        "    except:\n",
        "      with open('/content/drive/MyDrive/IR_Assignments/20_newsgroups/talk.politics.misc/'+str(file),encoding='ISO-8859-1') as doctry:\n",
        "        politicsData[file]=doctry.read()\n",
        "  if ctr%100==0:\n",
        "    print(ctr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heckDj5bGU2T",
        "outputId": "4a819b12-f6fa-4c2e-8b05-e90650ec239f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n"
          ]
        }
      ],
      "source": [
        "ctr=0\n",
        "for file in sports:\n",
        "  ctr+=1\n",
        "  with open('/content/drive/MyDrive/IR_Assignments/20_newsgroups/rec.sport.hockey/'+file) as doc:\n",
        "    try:\n",
        "      sportsData[file]=doc.read()\n",
        "\n",
        "    except:\n",
        "      with open('/content/drive/MyDrive/IR_Assignments/20_newsgroups/rec.sport.hockey/'+str(file),encoding='ISO-8859-1') as doctry:\n",
        "        sportsData[file]=doctry.read()\n",
        "  if ctr%100==0:\n",
        "    print(ctr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nRgsMoZGVFk",
        "outputId": "e6bbddff-f4c8-4d45-ec48-3d8c433e9e71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n"
          ]
        }
      ],
      "source": [
        "ctr=0\n",
        "for file in med:\n",
        "  ctr+=1\n",
        "  with open('/content/drive/MyDrive/IR_Assignments/20_newsgroups/sci.med/'+file) as doc:\n",
        "    try:\n",
        "      medData[file]=doc.read()\n",
        "\n",
        "    except:\n",
        "      with open('/content/drive/MyDrive/IR_Assignments/20_newsgroups/sci.med/'+str(file),encoding='ISO-8859-1') as doctry:\n",
        "        medData[file]=doctry.read()\n",
        "  if ctr%100==0:\n",
        "    print(ctr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrhoQeP8j0tV"
      },
      "source": [
        "Storing Class level json file which contains all class documents are there text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOMYxV2V01sW"
      },
      "outputs": [],
      "source": [
        "with open('graphicsData.json', 'w') as f:\n",
        "    json.dump(graphicsData, f)\n",
        "with open('sportsData.json', 'w') as f:\n",
        "    json.dump(sportsData, f)\n",
        "with open('medData.json', 'w') as f:\n",
        "    json.dump(medData, f)\n",
        "with open('spaceData.json', 'w') as f:\n",
        "    json.dump(spaceData, f)\n",
        "with open('politicsData.json', 'w') as f:\n",
        "    json.dump(politicsData, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8cSykFzNKXi"
      },
      "outputs": [],
      "source": [
        "politicsData = json.load(open(\"/content/drive/MyDrive/IR_Assignments/politicsData.json\", \"r\"))\n",
        "spaceData = json.load(open(\"/content/drive/MyDrive/IR_Assignments/spaceData.json\", \"r\"))\n",
        "sportsData = json.load(open(\"/content/drive/MyDrive/IR_Assignments/sportsData.json\", \"r\"))\n",
        "graphicsData = json.load(open(\"/content/drive/MyDrive/IR_Assignments/graphicsData.json\", \"r\"))\n",
        "medData = json.load(open(\"/content/drive/MyDrive/IR_Assignments/medData.json\", \"r\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IM2xj1ssj_2B"
      },
      "source": [
        "Generating doc level preprocessed data for all the classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUpaPf5JNGNw"
      },
      "outputs": [],
      "source": [
        "# preprocessing\n",
        "for doc in politicsData:\n",
        "    politicsData[doc] = preprocess(politicsData[doc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKNDiouENGwa"
      },
      "outputs": [],
      "source": [
        "# preprocessing\n",
        "for doc in spaceData:\n",
        "    spaceData[doc] = preprocess(spaceData[doc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VR71E97lNG92"
      },
      "outputs": [],
      "source": [
        "# preprocessing\n",
        "for doc in sportsData:\n",
        "    sportsData[doc] = preprocess(sportsData[doc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7KnlpCFNHex"
      },
      "outputs": [],
      "source": [
        "# preprocessing\n",
        "for doc in graphicsData:\n",
        "    graphicsData[doc] = preprocess(graphicsData[doc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ccp_Hy9pNHth"
      },
      "outputs": [],
      "source": [
        "# preprocessing\n",
        "for doc in medData:\n",
        "    medData[doc] = preprocess(medData[doc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ck7-aiVO_KV"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/IR_Assignments/graphicsDataProcLem.json', 'w') as f:\n",
        "    json.dump(graphicsData, f)\n",
        "with open('/content/drive/MyDrive/IR_Assignments/sportsDataProcLem.json', 'w') as f:\n",
        "    json.dump(sportsData, f)\n",
        "with open('/content/drive/MyDrive/IR_Assignments/medDataProcLem.json', 'w') as f:\n",
        "    json.dump(medData, f)\n",
        "with open('/content/drive/MyDrive/IR_Assignments/spaceDataProcLem.json', 'w') as f:\n",
        "    json.dump(spaceData, f)\n",
        "with open('/content/drive/MyDrive/IR_Assignments/politicsDataProcLem.json', 'w') as f:\n",
        "    json.dump(politicsData, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_OqfF9rkG3b"
      },
      "source": [
        "**Loading preprocessed files**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYJZaIDwg6A2"
      },
      "outputs": [],
      "source": [
        "# this is done to avoid doing preprocessing again and again\n",
        "politicsData = json.load(open(\"/content/drive/MyDrive/IR_Assignments/politicsDataProcLem.json\", \"r\"))\n",
        "spaceData = json.load(open(\"/content/drive/MyDrive/IR_Assignments/spaceDataProcLem.json\", \"r\"))\n",
        "sportsData = json.load(open(\"/content/drive/MyDrive/IR_Assignments/sportsDataProcLem.json\", \"r\"))\n",
        "graphicsData = json.load(open(\"/content/drive/MyDrive/IR_Assignments/graphicsDataProcLem.json\", \"r\"))\n",
        "medData = json.load(open(\"/content/drive/MyDrive/IR_Assignments/medDataProcLem.json\", \"r\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK9OhBFqkQsc"
      },
      "source": [
        "**Storing data in a structured manner**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TyUfzLol4sr",
        "outputId": "57c47073-211e-4e92-fcea-245b4226287a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n"
          ]
        }
      ],
      "source": [
        "# the below lists maps documents to their original classes\n",
        "politics_doc_list=[]\n",
        "sports_doc_list=[]\n",
        "med_doc_list=[]\n",
        "space_doc_list=[]\n",
        "graphics_doc_list=[]\n",
        "docsData={}\n",
        "#storing tokens for each document along with a self assigned class label\n",
        "for doc in politicsData:\n",
        "  politics_doc_list.append(doc+'p')\n",
        "  docsData[doc+'p']=[politicsData[doc],1]\n",
        "print(len(docsData))\n",
        "for doc in graphicsData:\n",
        "  graphics_doc_list.append(doc+'g')\n",
        "  docsData[doc+'g']=[graphicsData[doc],2]\n",
        "print(len(docsData))\n",
        "for doc in sportsData:\n",
        "  sports_doc_list.append(doc+'s')\n",
        "  docsData[doc+'s']=[sportsData[doc],3]\n",
        "print(len(docsData))\n",
        "for doc in spaceData:\n",
        "  space_doc_list.append(doc+'c')\n",
        "  docsData[doc+'c']=[spaceData[doc],4]\n",
        "print(len(docsData))\n",
        "for doc in medData:\n",
        "  med_doc_list.append(doc+'m')\n",
        "  docsData[doc+'m']=[medData[doc],5]\n",
        "print(len(docsData))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5Cf30lfkoDF"
      },
      "source": [
        "**Shuffling dataset before train test split**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEVHmu9ry11Z"
      },
      "outputs": [],
      "source": [
        "random.shuffle(politics_doc_list)\n",
        "random.shuffle(med_doc_list)\n",
        "random.shuffle(sports_doc_list)\n",
        "random.shuffle(space_doc_list)\n",
        "random.shuffle(graphics_doc_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysaDlyhKqo7Q"
      },
      "source": [
        "**Finding class priors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGRH_OZo9UQr"
      },
      "outputs": [],
      "source": [
        "def class_proba(trainx,trainy):\n",
        "  clsProb={}\n",
        "  classes=set(trainy)\n",
        "  for cls in classes:\n",
        "    clsProb[cls]=trainy.count(cls)/len(trainy)\n",
        "  return clsProb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06z6mkz5qs7M"
      },
      "source": [
        "**Training Multinomial Naive Bayes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9qmng5rS8DK"
      },
      "outputs": [],
      "source": [
        "def train_mnb(trainx,trainy,priors):\n",
        "  vocab_size=len(trainx[0])\n",
        "  weights={}\n",
        "  for cls in priors:\n",
        "    weights[cls]={}\n",
        "    for i in range(len(trainx[0])):\n",
        "      sum=0\n",
        "      for j in range(len(trainx)):\n",
        "        if trainy[j]==cls:\n",
        "          sum+=trainx[j][i]\n",
        "      weights[cls][i]=sum\n",
        "  for cls in weights:\n",
        "    tsum=0\n",
        "    for i in weights[cls]:\n",
        "      tsum=tsum+weights[cls][i]\n",
        "    for i in weights[cls]:\n",
        "      weights[cls][i]=(weights[cls][i]+1)/(tsum+vocab_size)   \n",
        "  return weights "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHkkKu7nq3kh"
      },
      "source": [
        "**Predicting results for a given data using the trained naive bayes weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngwnDjnEYEUX"
      },
      "outputs": [],
      "source": [
        "def predict(mnb,priors,testx):\n",
        "  pred=[]\n",
        "  for doc in testx:\n",
        "    # print(type(doc))\n",
        "    maxa=float('-inf')\n",
        "    var=-1\n",
        "    for cls in priors:\n",
        "      prob=math.log(priors[cls])\n",
        "      for i in range(len(doc)):\n",
        "        if doc[i]!=0:\n",
        "          prob=prob+doc[i]*math.log(mnb[cls][i])\n",
        "      # print(prob)\n",
        "      if prob>maxa:\n",
        "        maxa=prob\n",
        "        var=cls\n",
        "    pred.append(var)\n",
        "  return pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaEYqv4TrATY"
      },
      "source": [
        "**Finding accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcqhnneFpCfk"
      },
      "outputs": [],
      "source": [
        "def accuracy(l1,l2):\n",
        "  tot=0\n",
        "  true=0\n",
        "  if len(l1)!=len(l2):\n",
        "    return 0\n",
        "  for i in range(len(l1)):\n",
        "    if l1[i]==l2[i]:\n",
        "      true+=1\n",
        "    tot+=1\n",
        "  return true/tot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04xkpZdHrDeK"
      },
      "source": [
        "**Processing queries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpM-3q6EkyaV"
      },
      "outputs": [],
      "source": [
        "def process(k,tsize):\n",
        "\n",
        "  # Dividing docs to train and test based on the ratio given to the function\n",
        "  potrain, potest = politics_doc_list[0:int(tsize*len(politics_doc_list))],politics_doc_list[int(tsize*len(politics_doc_list)):]\n",
        "  grtrain, grtest = graphics_doc_list[0:int(tsize*len(graphics_doc_list))],graphics_doc_list[int(tsize*len(graphics_doc_list)):]\n",
        "  metrain, metest = med_doc_list[0:int(tsize*len(med_doc_list))],med_doc_list[int(tsize*len(med_doc_list)):]\n",
        "  sctrain, sctest = space_doc_list[0:int(tsize*len(space_doc_list))],space_doc_list[int(tsize*len(space_doc_list)):]\n",
        "  sptrain, sptest = sports_doc_list[0:int(tsize*len(sports_doc_list))],sports_doc_list[int(tsize*len(sports_doc_list)):]  \n",
        "  trainDocs=potrain+grtrain+metrain+sctrain+sptrain\n",
        "  testDocs=potest+grtest+metest+sctest+sptest\n",
        "  random.shuffle(trainDocs)\n",
        "  random.shuffle(testDocs)\n",
        "\n",
        "  #Storing class level information in order to find tf-icf values\n",
        "  politicsClass={}\n",
        "  graphicsClass={}\n",
        "  medClass={}\n",
        "  sportsClass={}\n",
        "  spaceClass={}\n",
        "  ctr=0\n",
        "  for doc in politicsData:\n",
        "    if doc+'p' in trainDocs:\n",
        "      for word in politicsData[doc]:\n",
        "        if word in politicsClass.keys():\n",
        "          politicsClass[word]+=1\n",
        "        else:\n",
        "          politicsClass[word]=1\n",
        "  for doc in medData:\n",
        "    if doc+'m' in trainDocs:\n",
        "      for word in medData[doc]:\n",
        "        if word in medClass.keys():\n",
        "          medClass[word]+=1\n",
        "        else:\n",
        "          medClass[word]=1\n",
        "  for doc in sportsData:\n",
        "    if doc+'s' in trainDocs:\n",
        "      for word in sportsData[doc]:\n",
        "        if word in sportsClass.keys():\n",
        "          sportsClass[word]+=1\n",
        "        else:\n",
        "          sportsClass[word]=1\n",
        "  for doc in spaceData:\n",
        "    if doc+'c' in trainDocs:\n",
        "      for word in spaceData[doc]:\n",
        "        if word in spaceClass.keys():\n",
        "          spaceClass[word]+=1\n",
        "        else:\n",
        "          spaceClass[word]=1\n",
        "  for doc in graphicsData:\n",
        "    if doc+'g' in trainDocs:\n",
        "      for word in graphicsData[doc]:\n",
        "        if word in graphicsClass.keys():\n",
        "          graphicsClass[word]+=1\n",
        "        else:\n",
        "          graphicsClass[word]=1\n",
        "  word_dict={}\n",
        "\n",
        "  # counting word frequencies in a class\n",
        "  for word in graphicsClass:\n",
        "    if word in word_dict.keys():\n",
        "      word_dict[word]+=1\n",
        "    else:\n",
        "      word_dict[word]=1\n",
        "  for word in sportsClass:\n",
        "    if word in word_dict.keys():\n",
        "      word_dict[word]+=1\n",
        "    else:\n",
        "      word_dict[word]=1\n",
        "  for word in medClass:\n",
        "    if word in word_dict.keys():\n",
        "      word_dict[word]+=1\n",
        "    else:\n",
        "      word_dict[word]=1\n",
        "  for word in spaceClass:\n",
        "    if word in word_dict.keys():\n",
        "      word_dict[word]+=1\n",
        "    else:\n",
        "      word_dict[word]=1\n",
        "  for word in politicsClass:\n",
        "    if word in word_dict.keys():\n",
        "      word_dict[word]+=1\n",
        "    else:\n",
        "      word_dict[word]=1\n",
        "  ctr=0\n",
        "\n",
        "  #storing tf-icf values\n",
        "  for word in word_dict:\n",
        "    icf=math.log2(5/word_dict[word])\n",
        "    if word in politicsClass.keys():\n",
        "      ctr+=1\n",
        "      politicsClass[word]=politicsClass[word]*icf\n",
        "    if word in graphicsClass.keys():\n",
        "      ctr+=1\n",
        "      graphicsClass[word]=graphicsClass[word]*icf\n",
        "    if word in medClass.keys():\n",
        "      ctr+=1\n",
        "      medClass[word]=medClass[word]*icf\n",
        "    if word in spaceClass.keys():\n",
        "      ctr+=1\n",
        "      spaceClass[word]=spaceClass[word]*icf\n",
        "    if word in sportsClass.keys():\n",
        "      ctr+=1\n",
        "      sportsClass[word]=sportsClass[word]*icf\n",
        "  \n",
        "  #Getting top features for each class by sorting them in reverse order  \n",
        "  politicsClass=sorted(politicsClass.items(), key=lambda value: value[1])\n",
        "  graphicsClass=sorted(graphicsClass.items(), key=lambda value: value[1])\n",
        "  medClass=sorted(medClass.items(), key=lambda value: value[1])\n",
        "  sportsClass=sorted(sportsClass.items(), key=lambda value: value[1])\n",
        "  spaceClass=sorted(spaceClass.items(), key=lambda value: value[1])\n",
        "  politicsClass.reverse()\n",
        "  graphicsClass.reverse()\n",
        "  medClass.reverse()\n",
        "  sportsClass.reverse()\n",
        "  spaceClass.reverse()\n",
        "  # making a feature set of union of top k features of each class basedon tf-icf values\n",
        "  features=set()\n",
        "  ctr=0\n",
        "  for word in politicsClass:\n",
        "    if k==0:\n",
        "      break\n",
        "    ctr+=1\n",
        "    features.add(word[0])\n",
        "    if ctr>=k:\n",
        "      break\n",
        "  ctr=0\n",
        "  for word in medClass:\n",
        "    if k==0:\n",
        "      break\n",
        "    ctr+=1\n",
        "    features.add(word[0])\n",
        "    if ctr>=k:\n",
        "      break\n",
        "  ctr=0\n",
        "  for word in graphicsClass:\n",
        "    if k==0:\n",
        "      break\n",
        "    ctr+=1\n",
        "    features.add(word[0])\n",
        "    if ctr>=k:\n",
        "      break\n",
        "  ctr=0\n",
        "  for word in sportsClass:\n",
        "    if k==0:\n",
        "      break\n",
        "    ctr+=1\n",
        "    features.add(word[0])\n",
        "    if ctr>=k:\n",
        "      break\n",
        "  ctr=0\n",
        "  for word in spaceClass:\n",
        "    if k==0:\n",
        "      break\n",
        "    ctr+=1\n",
        "    features.add(word[0])\n",
        "    if ctr>=k:\n",
        "      break\n",
        "  features=list(features)\n",
        "\n",
        "  #creating a train test dataset that can be used for model training and evaluating (based on word count of doc)\n",
        "  trainx,trainy,testx,testy=[],[],[],[]\n",
        "  for doc in trainDocs:\n",
        "    tmp=[]\n",
        "    for word in features:\n",
        "      tmp.append(docsData[doc][0].count(word))\n",
        "    trainx.append(tmp)\n",
        "    trainy.append(docsData[doc][1])\n",
        "  for doc in testDocs:\n",
        "    tmp=[]\n",
        "    for word in features:\n",
        "      tmp.append(docsData[doc][0].count(word))\n",
        "    testx.append(tmp)\n",
        "    testy.append(docsData[doc][1])\n",
        "  \n",
        "  #Training, testing and reporting accuracies\n",
        "  priors=class_proba(trainx,trainy)\n",
        "  mnb=train_mnb(trainx,trainy,priors)\n",
        "  preds=predict(mnb,priors,trainx)\n",
        "  print('Train Accuracy =',accuracy(trainy,preds))\n",
        "  preds=predict(mnb,priors,testx)\n",
        "  print()\n",
        "  print('Test Accuracy =',accuracy(testy,preds))\n",
        "  print()\n",
        "  clf_report(testy,preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-tb34uYoPkR"
      },
      "outputs": [],
      "source": [
        "def clf_report(testy,preds):\n",
        "  matrix={}\n",
        "  count1=set(testy)\n",
        "  count2=set(preds)\n",
        "  print(count1)\n",
        "  print(count2)\n",
        "  for i in count1:\n",
        "    matrix[i]={}\n",
        "    for j in count2:\n",
        "      matrix[i][j]=0\n",
        "  for i in range(len(testy)):\n",
        "    matrix[testy[i]][preds[i]]+=1\n",
        "  lst=[]\n",
        "  for i in matrix:\n",
        "    temp=[]\n",
        "    for j in matrix[i]:\n",
        "      temp.append(matrix[i][j])\n",
        "    lst.append(temp)\n",
        "  print(lst)\n",
        "  df = pd.DataFrame(lst,index=[1,2,3,4,5],columns=[1,2,3,4,5])\n",
        "  print('Confusion Matrix for Test Samples is as follows:')\n",
        "  print(df)\n",
        "  for i in count1:\n",
        "    tot=0\n",
        "    tot2=0\n",
        "    for j in count2:\n",
        "      tot+=matrix[i][j]\n",
        "    for k in count2:\n",
        "      tot2+=matrix[k][i]\n",
        "    recall=matrix[i][i]/tot2\n",
        "    precision=matrix[i][i]/tot\n",
        "    print('Precision for Class',i,'=',precision)\n",
        "    print('Recall for Class',i,'=',recall)\n",
        "    print('F1 for Class',i,'=',2*precision*recall/(precision+recall))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN7pH0XVoX64"
      },
      "source": [
        "Running the code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1hAZy652mPtf"
      },
      "outputs": [],
      "source": [
        "k=int(input('Enter value of k:'))\n",
        "split={'50:50 Split':0.5,'70:30 Split':0.7,'80:20 Split':0.8}\n",
        "for i in split:\n",
        "  print('\\n\\nResults for',i)\n",
        "  process(k,split[i])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NaiveBayes.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}